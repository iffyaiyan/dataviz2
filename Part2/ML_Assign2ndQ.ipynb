{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUrQQV53grmw",
        "outputId": "9541685e-761e-4749-9ff3-2506b802b634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MNIST dataset...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training data shape: (60000, 28, 28, 1)\n",
            "Training labels shape: (60000, 10)\n",
            "Test data shape: (10000, 28, 28, 1)\n",
            "Test labels shape: (10000, 10)\n",
            "\n",
            "=== Training Baseline Model ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - accuracy: 0.7814 - loss: 0.7056 - val_accuracy: 0.9632 - val_loss: 0.1253\n",
            "Epoch 2/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9413 - loss: 0.1924 - val_accuracy: 0.9728 - val_loss: 0.0920\n",
            "Epoch 3/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9577 - loss: 0.1376 - val_accuracy: 0.9773 - val_loss: 0.0760\n",
            "Epoch 4/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9667 - loss: 0.1080 - val_accuracy: 0.9773 - val_loss: 0.0752\n",
            "Epoch 5/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9721 - loss: 0.0926 - val_accuracy: 0.9785 - val_loss: 0.0717\n",
            "Epoch 6/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9748 - loss: 0.0821 - val_accuracy: 0.9795 - val_loss: 0.0736\n",
            "Epoch 7/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.9762 - loss: 0.0740 - val_accuracy: 0.9823 - val_loss: 0.0621\n",
            "Epoch 8/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9809 - loss: 0.0645 - val_accuracy: 0.9833 - val_loss: 0.0625\n",
            "Epoch 9/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9822 - loss: 0.0582 - val_accuracy: 0.9825 - val_loss: 0.0623\n",
            "Epoch 10/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9826 - loss: 0.0550 - val_accuracy: 0.9823 - val_loss: 0.0646\n",
            "Epoch 11/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9828 - loss: 0.0520 - val_accuracy: 0.9823 - val_loss: 0.0682\n",
            "Epoch 12/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9842 - loss: 0.0457 - val_accuracy: 0.9820 - val_loss: 0.0688\n",
            "Baseline Test Accuracy: 0.9806\n",
            "Baseline Training Time: 81.47 seconds\n",
            "Baseline Inference Time: 0.68 seconds\n",
            "\n",
            "=== Analyzing Baseline Results ===\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Accuracy: 0.9806\n",
            "Precision: 0.9806\n",
            "Recall: 0.9804\n",
            "F1 Score: 0.9805\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9848    0.9929    0.9888       980\n",
            "           1     0.9869    0.9938    0.9903      1135\n",
            "           2     0.9778    0.9806    0.9792      1032\n",
            "           3     0.9792    0.9782    0.9787      1010\n",
            "           4     0.9856    0.9776    0.9816       982\n",
            "           5     0.9820    0.9787    0.9803       892\n",
            "           6     0.9822    0.9812    0.9817       958\n",
            "           7     0.9719    0.9767    0.9743      1028\n",
            "           8     0.9784    0.9743    0.9763       974\n",
            "           9     0.9770    0.9703    0.9736      1009\n",
            "\n",
            "    accuracy                         0.9806     10000\n",
            "   macro avg     0.9806    0.9804    0.9805     10000\n",
            "weighted avg     0.9806    0.9806    0.9806     10000\n",
            "\n",
            "\n",
            "=== Training Local-Global Feature Model ===\n",
            "Input images shape: (1, 28, 28, 1)\n",
            "Patches shape: (1, 16, 7, 7, 1)\n",
            "Flat patches shape: (1, 16, 49)\n",
            "Input images shape: (1, 28, 28, 1)\n",
            "Patches shape: (1, 16, 7, 7, 1)\n",
            "Flat patches shape: (1, 16, 49)\n",
            "Input images shape: (1, 28, 28, 1)\n",
            "Patches shape: (1, 16, 7, 7, 1)\n",
            "Flat patches shape: (1, 16, 49)\n",
            "Epoch 1/20\n",
            "Input images shape: (None, 28, 28, 1)\n",
            "Patches shape: (None, 16, None, None, 1)\n",
            "Flat patches shape: (None, 16, 49)\n",
            "Input images shape: (None, 28, 28, 1)\n",
            "Patches shape: (None, 16, None, None, 1)\n",
            "Flat patches shape: (None, 16, 49)\n",
            "Input images shape: (None, 28, 28, 1)\n",
            "Patches shape: (None, 16, None, None, 1)\n",
            "Flat patches shape: (None, 16, 49)\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.3178 - loss: 1.9465Input images shape: (None, 28, 28, 1)\n",
            "Patches shape: (None, 16, None, None, 1)\n",
            "Flat patches shape: (None, 16, 49)\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 48ms/step - accuracy: 0.3181 - loss: 1.9457 - val_accuracy: 0.6158 - val_loss: 1.1322 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 49ms/step - accuracy: 0.6267 - loss: 1.0995 - val_accuracy: 0.6990 - val_loss: 0.8940 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.6936 - loss: 0.9067 - val_accuracy: 0.7417 - val_loss: 0.7846 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 47ms/step - accuracy: 0.7391 - loss: 0.7767 - val_accuracy: 0.7738 - val_loss: 0.6934 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 50ms/step - accuracy: 0.7537 - loss: 0.7424 - val_accuracy: 0.7928 - val_loss: 0.6151 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 46ms/step - accuracy: 0.7734 - loss: 0.6826 - val_accuracy: 0.8112 - val_loss: 0.5635 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 49ms/step - accuracy: 0.7821 - loss: 0.6587 - val_accuracy: 0.8330 - val_loss: 0.5110 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - accuracy: 0.7976 - loss: 0.6154 - val_accuracy: 0.8268 - val_loss: 0.5382 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 45ms/step - accuracy: 0.8090 - loss: 0.5806 - val_accuracy: 0.8328 - val_loss: 0.5137 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.8134 - loss: 0.5692 - val_accuracy: 0.8613 - val_loss: 0.4324 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - accuracy: 0.8179 - loss: 0.5542 - val_accuracy: 0.8517 - val_loss: 0.4551 - learning_rate: 0.0010\n",
            "Epoch 12/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 53ms/step - accuracy: 0.8260 - loss: 0.5321 - val_accuracy: 0.8313 - val_loss: 0.5223 - learning_rate: 0.0010\n",
            "Epoch 13/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 46ms/step - accuracy: 0.8260 - loss: 0.5184 - val_accuracy: 0.8462 - val_loss: 0.4895 - learning_rate: 0.0010\n",
            "Epoch 14/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 51ms/step - accuracy: 0.8364 - loss: 0.4946 - val_accuracy: 0.8758 - val_loss: 0.3940 - learning_rate: 5.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 49ms/step - accuracy: 0.8477 - loss: 0.4657 - val_accuracy: 0.8742 - val_loss: 0.3935 - learning_rate: 5.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 48ms/step - accuracy: 0.8396 - loss: 0.4838 - val_accuracy: 0.8758 - val_loss: 0.3792 - learning_rate: 5.0000e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.8533 - loss: 0.4466 - val_accuracy: 0.8640 - val_loss: 0.4178 - learning_rate: 5.0000e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 51ms/step - accuracy: 0.8553 - loss: 0.4477 - val_accuracy: 0.8672 - val_loss: 0.4159 - learning_rate: 5.0000e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 50ms/step - accuracy: 0.8495 - loss: 0.4486 - val_accuracy: 0.8613 - val_loss: 0.4301 - learning_rate: 5.0000e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 49ms/step - accuracy: 0.8653 - loss: 0.4069 - val_accuracy: 0.8910 - val_loss: 0.3283 - learning_rate: 2.5000e-04\n",
            "Test accuracy: 0.8827\n",
            "Training time: 557.44 seconds\n",
            "Inference time on test set: 1.79 seconds\n",
            "\n",
            "=== Analyzing Local-Global Model Results ===\n",
            "Input images shape: (32, 28, 28, 1)\n",
            "Patches shape: (32, 16, None, None, 1)\n",
            "Flat patches shape: (32, 16, 49)\n",
            "\u001b[1m306/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/stepInput images shape: (None, 28, 28, 1)\n",
            "Patches shape: (None, 16, None, None, 1)\n",
            "Flat patches shape: (None, 16, 49)\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step\n",
            "Accuracy: 0.8761\n",
            "Precision: 0.8751\n",
            "Recall: 0.8749\n",
            "F1 Score: 0.8746\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9114    0.9031    0.9072       980\n",
            "           1     0.9574    0.9692    0.9632      1135\n",
            "           2     0.8280    0.8072    0.8175      1032\n",
            "           3     0.8669    0.8960    0.8812      1010\n",
            "           4     0.8919    0.9155    0.9035       982\n",
            "           5     0.8188    0.8408    0.8296       892\n",
            "           6     0.8863    0.8622    0.8741       958\n",
            "           7     0.9113    0.8492    0.8792      1028\n",
            "           8     0.8145    0.8789    0.8454       974\n",
            "           9     0.8642    0.8266    0.8450      1009\n",
            "\n",
            "    accuracy                         0.8761     10000\n",
            "   macro avg     0.8751    0.8749    0.8746     10000\n",
            "weighted avg     0.8767    0.8761    0.8760     10000\n",
            "\n",
            "\n",
            "=== Comparing Models ===\n",
            "\n",
            "=== Generating Mathematical Model Report ===\n",
            "\n",
            "Experiment complete. Results and visualizations saved to disk.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Configuration class to hold hyperparameters\n",
        "class Config:\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 20\n",
        "    LEARNING_RATE = 0.001\n",
        "    PATCH_SIZE = 7  # Size of local patches\n",
        "    NUM_PATCHES = 16  # Number of patches to extract\n",
        "    LOCAL_FEATURE_DIM = 32  # Dimension of local features\n",
        "    GLOBAL_FEATURE_DIM = 128  # Dimension of global features\n",
        "    HIDDEN_UNITS = 256  # Hidden units in fully connected layer\n",
        "    DROPOUT_RATE = 0.3  # Dropout rate\n",
        "    IMG_SIZE = 28  # MNIST image size\n",
        "    NUM_CLASSES = 10  # Number of digit classes\n",
        "    VALIDATION_SPLIT = 0.1  # Percentage of training data to use for validation\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Function to load and preprocess the MNIST dataset\n",
        "def load_and_preprocess_data():\n",
        "    print(\"Loading MNIST dataset...\")\n",
        "    # Load MNIST dataset from TensorFlow\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Normalize pixel values to [0, 1]\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "    # Reshape to add channel dimension (required for TensorFlow)\n",
        "    x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "    x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "    print(f\"Training data shape: {x_train.shape}\")\n",
        "    print(f\"Training labels shape: {y_train.shape}\")\n",
        "    print(f\"Test data shape: {x_test.shape}\")\n",
        "    print(f\"Test labels shape: {y_test.shape}\")\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "# Extract local patches from images\n",
        "# class PatchExtractor(tf.keras.layers.Layer):\n",
        "#     def __init__(self, patch_size, num_patches):\n",
        "#         super(PatchExtractor, self).__init__()\n",
        "#         self.patch_size = patch_size\n",
        "#         self.num_patches = num_patches\n",
        "\n",
        "#     def call(self, images):\n",
        "#         batch_size = tf.shape(images)[0]\n",
        "\n",
        "#         # Initialize an array to store our patches\n",
        "#         patches = []\n",
        "\n",
        "#         for _ in range(self.num_patches):\n",
        "#             # Generate random starting positions for patches\n",
        "#             h_start = tf.random.uniform(shape=[], maxval=config.IMG_SIZE - self.patch_size, dtype=tf.int32)\n",
        "#             w_start = tf.random.uniform(shape=[], maxval=config.IMG_SIZE - self.patch_size, dtype=tf.int32)\n",
        "\n",
        "#             # Extract patch\n",
        "#             patch = images[:, h_start:h_start + self.patch_size, w_start:w_start + self.patch_size, :]\n",
        "#             patches.append(patch)\n",
        "\n",
        "#         # Stack all patches along a new dimension\n",
        "#         patches = tf.stack(patches, axis=1)  # Shape: [batch_size, num_patches, patch_size, patch_size, channels]\n",
        "\n",
        "#         # Flatten each patch\n",
        "#         flat_patches = tf.reshape(patches, [batch_size, self.num_patches, self.patch_size * self.patch_size])\n",
        "\n",
        "#         return flat_patches\n",
        "\n",
        "# class PatchExtractor(tf.keras.layers.Layer):\n",
        "#     def __init__(self, patch_size, num_patches):\n",
        "#         super(PatchExtractor, self).__init__()\n",
        "#         self.patch_size = patch_size\n",
        "#         self.num_patches = num_patches\n",
        "\n",
        "#     def call(self, images):\n",
        "#         batch_size = tf.shape(images)[0]\n",
        "\n",
        "#         # Initialize an array to store our patches\n",
        "#         patches = []\n",
        "\n",
        "#         for _ in range(self.num_patches):\n",
        "#             # Generate random starting positions for patches\n",
        "#             h_start = tf.random.uniform(shape=[], maxval=config.IMG_SIZE - self.patch_size, dtype=tf.int32)\n",
        "#             w_start = tf.random.uniform(shape=[], maxval=config.IMG_SIZE - self.patch_size, dtype=tf.int32)\n",
        "\n",
        "#             # Extract patch\n",
        "#             patch = images[:, h_start:h_start + self.patch_size, w_start:w_start + self.patch_size, :]\n",
        "#             patches.append(patch)\n",
        "\n",
        "#         # Stack all patches along a new dimension\n",
        "#         patches = tf.stack(patches, axis=1)  # Shape: [batch_size, num_patches, patch_size, patch_size, channels]\n",
        "\n",
        "#         # Flatten each patch\n",
        "#         flat_patches = tf.reshape(patches, [batch_size, self.num_patches, self.patch_size * self.patch_size * patches.shape[-1]])\n",
        "\n",
        "#         return flat_patches\n",
        "\n",
        "\n",
        "# class PatchExtractor(tf.keras.layers.Layer):\n",
        "#     def __init__(self, patch_size, num_patches):\n",
        "#         super(PatchExtractor, self).__init__()\n",
        "#         self.patch_size = patch_size\n",
        "#         self.num_patches = num_patches\n",
        "\n",
        "#     def call(self, images):\n",
        "#         batch_size = tf.shape(images)[0]\n",
        "\n",
        "#         # Initialize an array to store our patches\n",
        "#         patches = []\n",
        "\n",
        "#         for _ in range(self.num_patches):\n",
        "#             # Generate random starting positions for patches\n",
        "#             h_start = tf.random.uniform(shape=[], maxval=config.IMG_SIZE - self.patch_size, dtype=tf.int32)\n",
        "#             w_start = tf.random.uniform(shape=[], maxval=config.IMG_SIZE - self.patch_size, dtype=tf.int32)\n",
        "\n",
        "#             # Extract patch\n",
        "#             patch = images[:, h_start:h_start + self.patch_size, w_start:w_start + self.patch_size, :]\n",
        "#             patches.append(patch)\n",
        "\n",
        "#         # Stack all patches along a new dimension\n",
        "#         patches = tf.stack(patches, axis=1)  # Shape: [batch_size, num_patches, patch_size, patch_size, channels]\n",
        "\n",
        "#         # Flatten each patch\n",
        "#         flat_patches = tf.reshape(patches, [batch_size, self.num_patches, self.patch_size * self.patch_size * patches.shape[-1]])\n",
        "\n",
        "#         return flat_patches\n",
        "\n",
        "class PatchExtractor(tf.keras.layers.Layer):\n",
        "    def __init__(self, patch_size, num_patches):\n",
        "        super(PatchExtractor, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "\n",
        "        # Initialize an array to store our patches\n",
        "        patches = []\n",
        "\n",
        "        for _ in range(self.num_patches):\n",
        "            # Generate random starting positions for patches\n",
        "            h_start = tf.random.uniform(shape=[], maxval=config.IMG_SIZE - self.patch_size, dtype=tf.int32)\n",
        "            w_start = tf.random.uniform(shape=[], maxval=config.IMG_SIZE - self.patch_size, dtype=tf.int32)\n",
        "\n",
        "            # Extract patch\n",
        "            patch = images[:, h_start:h_start + self.patch_size, w_start:w_start + self.patch_size, :]\n",
        "            patches.append(patch)\n",
        "\n",
        "        # Stack all patches along a new dimension\n",
        "        patches = tf.stack(patches, axis=1)  # Shape: [batch_size, num_patches, patch_size, patch_size, channels]\n",
        "\n",
        "        # Debug: Print shapes\n",
        "        print(f\"Input images shape: {images.shape}\")\n",
        "        print(f\"Patches shape: {patches.shape}\")\n",
        "\n",
        "        # Flatten each patch\n",
        "        flat_patches = tf.reshape(patches, [batch_size, self.num_patches, self.patch_size * self.patch_size * patches.shape[-1]])\n",
        "\n",
        "        # Debug: Print flattened patches shape\n",
        "        print(f\"Flat patches shape: {flat_patches.shape}\")\n",
        "\n",
        "        return flat_patches\n",
        "\n",
        "# Local Feature Encoder\n",
        "class LocalFeatureEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, feature_dim):\n",
        "        super(LocalFeatureEncoder, self).__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(feature_dim, activation='relu')\n",
        "        self.norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self, patches):\n",
        "        # Process each patch individually\n",
        "        x = self.dense1(patches)\n",
        "        x = self.dense2(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "# Global Feature Aggregator\n",
        "class GlobalFeatureAggregator(tf.keras.layers.Layer):\n",
        "    def __init__(self, feature_dim):\n",
        "        super(GlobalFeatureAggregator, self).__init__()\n",
        "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=4, key_dim=32)\n",
        "        self.dense = tf.keras.layers.Dense(feature_dim, activation='relu')\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self, local_features):\n",
        "        # Self-attention to capture relationships between patches\n",
        "        attention_output = self.attention(local_features, local_features)\n",
        "        x = self.norm1(local_features + attention_output)\n",
        "\n",
        "        # Process the attended features further\n",
        "        global_features = self.dense(x)\n",
        "        global_features = self.norm2(global_features)\n",
        "\n",
        "        # Combine all patch features to get a single global representation\n",
        "        # through learnable aggregation (attention-weighted sum)\n",
        "        global_representation = tf.reduce_mean(global_features, axis=1)\n",
        "\n",
        "        return global_representation\n",
        "\n",
        "# The full model combining local and global feature extraction\n",
        "class LocalGlobalDigitClassifier(tf.keras.Model):\n",
        "    def __init__(self, config):\n",
        "        super(LocalGlobalDigitClassifier, self).__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Patch extraction layer\n",
        "        self.patch_extractor = PatchExtractor(\n",
        "            patch_size=config.PATCH_SIZE,\n",
        "            num_patches=config.NUM_PATCHES\n",
        "        )\n",
        "\n",
        "        # Local feature encoder\n",
        "        self.local_encoder = LocalFeatureEncoder(\n",
        "            feature_dim=config.LOCAL_FEATURE_DIM\n",
        "        )\n",
        "\n",
        "        # Global feature aggregator\n",
        "        self.global_aggregator = GlobalFeatureAggregator(\n",
        "            feature_dim=config.GLOBAL_FEATURE_DIM\n",
        "        )\n",
        "\n",
        "        # Final classification layers\n",
        "        self.dense1 = tf.keras.layers.Dense(config.HIDDEN_UNITS, activation='relu')\n",
        "        self.dropout = tf.keras.layers.Dropout(config.DROPOUT_RATE)\n",
        "        self.norm = tf.keras.layers.LayerNormalization()\n",
        "        self.classifier = tf.keras.layers.Dense(config.NUM_CLASSES, activation='softmax')\n",
        "\n",
        "    def call(self, images, training=False):\n",
        "        # Extract patches from images\n",
        "        patches = self.patch_extractor(images)\n",
        "\n",
        "        # Encode local features from patches\n",
        "        local_features = self.local_encoder(patches)\n",
        "\n",
        "        # Aggregate local features to capture global structure\n",
        "        global_representation = self.global_aggregator(local_features)\n",
        "\n",
        "        # Classification based on the global representation\n",
        "        x = self.dense1(global_representation)\n",
        "        x = self.dropout(x, training=training)\n",
        "        x = self.norm(x)\n",
        "        output = self.classifier(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Simple baseline model for comparison\n",
        "def create_baseline_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Function to visualize the patches extracted by the model\n",
        "def visualize_patches(model, images, num_samples=3):\n",
        "    # Extract patches using the model's patch extractor\n",
        "    patch_extractor = model.patch_extractor\n",
        "\n",
        "    plt.figure(figsize=(15, num_samples * 3))\n",
        "    for i in range(num_samples):\n",
        "        # Get a single image\n",
        "        image = images[i:i+1]\n",
        "\n",
        "        # Extract patches\n",
        "        patches = patch_extractor(image)\n",
        "        patches_reshaped = tf.reshape(patches,\n",
        "                                     [config.NUM_PATCHES,\n",
        "                                      config.PATCH_SIZE,\n",
        "                                      config.PATCH_SIZE])\n",
        "\n",
        "        # Plot original image\n",
        "        plt.subplot(num_samples, config.NUM_PATCHES + 1, i * (config.NUM_PATCHES + 1) + 1)\n",
        "        plt.imshow(tf.squeeze(image), cmap='gray')\n",
        "        plt.title(f\"Image {i}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Plot patches\n",
        "        for j in range(config.NUM_PATCHES):\n",
        "            plt.subplot(num_samples, config.NUM_PATCHES + 1, i * (config.NUM_PATCHES + 1) + j + 2)\n",
        "            plt.imshow(patches_reshaped[j], cmap='gray')\n",
        "            plt.title(f\"Patch {j}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"patches_visualization.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Training function\n",
        "def train_model(model, x_train, y_train, x_test, y_test, config):\n",
        "    # Compile the model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=config.LEARNING_RATE)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Define callbacks\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        epochs=config.EPOCHS,\n",
        "        validation_split=config.VALIDATION_SPLIT,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_start_time = time.time()\n",
        "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "    test_time = time.time() - test_start_time\n",
        "\n",
        "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Training time: {training_time:.2f} seconds\")\n",
        "    print(f\"Inference time on test set: {test_time:.2f} seconds\")\n",
        "\n",
        "    return history, test_accuracy, training_time, test_time\n",
        "\n",
        "# Function to analyze predictions and compute metrics\n",
        "def analyze_results(model, x_test, y_test):\n",
        "    # Get predictions\n",
        "    y_pred_probs = model.predict(x_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(y_true, y_pred, digits=4)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.savefig(\"confusion_matrix.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Plot examples of misclassifications\n",
        "    misclassified_indices = np.where(y_pred != y_true)[0]\n",
        "    if len(misclassified_indices) > 0:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        num_samples = min(10, len(misclassified_indices))\n",
        "        for i in range(num_samples):\n",
        "            idx = misclassified_indices[i]\n",
        "            plt.subplot(2, 5, i + 1)\n",
        "            plt.imshow(np.squeeze(x_test[idx]), cmap='gray')\n",
        "            plt.title(f\"True: {y_true[idx]}, Pred: {y_pred[idx]}\")\n",
        "            plt.axis('off')\n",
        "        plt.savefig(\"misclassified_examples.png\")\n",
        "        plt.close()\n",
        "\n",
        "    # Analyze class-wise performance\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    class_precision = precision_score(y_true, y_pred, average=None)\n",
        "    class_recall = recall_score(y_true, y_pred, average=None)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(range(10), class_precision)\n",
        "    plt.title('Precision by Class')\n",
        "    plt.xlabel('Digit')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.ylim([0, 1])\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(range(10), class_recall)\n",
        "    plt.title('Recall by Class')\n",
        "    plt.xlabel('Digit')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.ylim([0, 1])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"class_performance.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'confusion_matrix': cm,\n",
        "        'misclassified_indices': misclassified_indices\n",
        "    }\n",
        "\n",
        "# Plot training history\n",
        "def plot_training_history(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"training_history.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "  # Compare models function\n",
        "def compare_models(custom_results, baseline_results):\n",
        "    models = ['Baseline', 'Local-Global Model']\n",
        "    metrics = {\n",
        "        'accuracy': [baseline_results['accuracy'], custom_results['accuracy']],\n",
        "        'precision': [baseline_results['precision'], custom_results['precision']],\n",
        "        'recall': [baseline_results['recall'], custom_results['recall']],\n",
        "        'f1': [baseline_results['f1'], custom_results['f1']],\n",
        "        'training_time': [baseline_results['training_time'], custom_results['training_time']],\n",
        "        'inference_time': [baseline_results['test_time'], custom_results['test_time']]\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Performance metrics\n",
        "    plt.subplot(2, 2, 1)\n",
        "    x = np.arange(len(models))\n",
        "    width = 0.2\n",
        "    plt.bar(x - 1.5*width, metrics['accuracy'], width, label='Accuracy')\n",
        "    plt.bar(x - 0.5*width, metrics['precision'], width, label='Precision')\n",
        "    plt.bar(x + 0.5*width, metrics['recall'], width, label='Recall')\n",
        "    plt.bar(x + 1.5*width, metrics['f1'], width, label='F1')\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Performance Metrics')\n",
        "    plt.xticks(x, models)\n",
        "    plt.ylim([0, 1])\n",
        "    plt.legend()\n",
        "\n",
        "    # Training and inference time\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.bar(x - 0.15, metrics['training_time'], width=0.3, label='Training Time (s)')\n",
        "    plt.bar(x + 0.15, metrics['inference_time'], width=0.3, label='Inference Time (s)')\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Time (s)')\n",
        "    plt.title('Computational Efficiency')\n",
        "    plt.xticks(x, models)\n",
        "    plt.legend()\n",
        "\n",
        "    # Confusion matrix differences\n",
        "    plt.subplot(2, 1, 2)\n",
        "    diff_cm = custom_results['confusion_matrix'] - baseline_results['confusion_matrix']\n",
        "    sns.heatmap(diff_cm, annot=True, fmt='d', cmap='coolwarm', center=0)\n",
        "    plt.title('Confusion Matrix Difference (Local-Global - Baseline)')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"model_comparison.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Generate report on mathematical model and approach\n",
        "def generate_model_report():\n",
        "    report = \"\"\"\n",
        "    # Mathematical Model Report: Local-Global Feature Architecture for MNIST Classification\n",
        "\n",
        "    ## 1. Introduction\n",
        "    This report details a novel neural network architecture designed to leverage both local and global features\n",
        "    in image data for improved classification of handwritten digits from the MNIST dataset.\n",
        "\n",
        "    ## 2. Mathematical Formulation\n",
        "\n",
        "    ### 2.1 Local Feature Extraction\n",
        "\n",
        "    Given an input image X ∈ ℝ^(28×28), we extract N patches {p_1, p_2, ..., p_N} where each\n",
        "    p_i ∈ ℝ^(k×k) represents a local region of the image (k = patch size).\n",
        "\n",
        "    For each patch p_i, we compute a local feature representation:\n",
        "\n",
        "    f_i = φ(W_2 · ReLU(W_1 · flatten(p_i) + b_1) + b_2)\n",
        "\n",
        "    where:\n",
        "    - flatten(p_i) ∈ ℝ^(k²) is the flattened patch\n",
        "    - W_1 ∈ ℝ^(64×k²), b_1 ∈ ℝ^64 are parameters of the first dense layer\n",
        "    - W_2 ∈ ℝ^(d_local×64), b_2 ∈ ℝ^d_local are parameters of the second dense layer\n",
        "    - φ represents the layer normalization operation\n",
        "    - d_local is the local feature dimension\n",
        "\n",
        "    This produces a set of local feature vectors F = {f_1, f_2, ..., f_N} where each f_i ∈ ℝ^d_local.\n",
        "\n",
        "    ### 2.2 Global Feature Aggregation\n",
        "\n",
        "    To capture relationships between local features and learn global structure, we employ a self-attention mechanism:\n",
        "\n",
        "    A = MultiHeadAttention(F, F)\n",
        "    F' = LayerNorm(F + A)\n",
        "\n",
        "    The multi-head attention operation can be expressed as:\n",
        "\n",
        "    MultiHeadAttention(Q, K, V) = Concat(head_1, ..., head_h)W^O\n",
        "\n",
        "    where each head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n",
        "\n",
        "    The attention function is:\n",
        "\n",
        "    Attention(Q, K, V) = softmax((QK^T)/√d_k)V\n",
        "\n",
        "    After applying attention, we further process the attended features:\n",
        "\n",
        "    G = LayerNorm(W_G · F' + b_G)\n",
        "\n",
        "    where W_G ∈ ℝ^(d_global×d_local), b_G ∈ ℝ^d_global are learnable parameters, and d_global is the global feature dimension.\n",
        "\n",
        "    Finally, we aggregate the global features into a single representation:\n",
        "\n",
        "    g = 1/N · ∑_i=1^N G_i\n",
        "\n",
        "    where g ∈ ℝ^d_global is the final global representation.\n",
        "\n",
        "    ### 2.3 Classification\n",
        "\n",
        "    The global representation g is then passed through a final classification network:\n",
        "\n",
        "    h = ReLU(W_h · g + b_h)\n",
        "    h' = Dropout(h)\n",
        "    h'' = LayerNorm(h')\n",
        "    y = softmax(W_y · h'' + b_y)\n",
        "\n",
        "    where:\n",
        "    - W_h ∈ ℝ^(d_hidden×d_global), b_h ∈ ℝ^d_hidden are parameters of the hidden layer\n",
        "    - W_y ∈ ℝ^(10×d_hidden), b_y ∈ ℝ^10 are parameters of the output layer\n",
        "    - y ∈ ℝ^10 is the final class probability distribution\n",
        "\n",
        "    ## 3. Model Training\n",
        "\n",
        "    The model is trained using categorical cross-entropy loss:\n",
        "\n",
        "    L(y, ŷ) = -∑_c y_c log(ŷ_c)\n",
        "\n",
        "    where y is the one-hot encoded true label and ŷ is the predicted probability distribution.\n",
        "\n",
        "    The optimization is performed using Adam with an initial learning rate of 0.001,\n",
        "    which is adaptively reduced when validation loss plateaus.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(\"model_report.md\", \"w\") as f:\n",
        "        f.write(report)\n",
        "\n",
        "    return report\n",
        "\n",
        "# Main function to run the experiment\n",
        "def main():\n",
        "    # Create results directory\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    # Load and preprocess data\n",
        "    (x_train, y_train), (x_test, y_test) = load_and_preprocess_data()\n",
        "\n",
        "    # Create and train the baseline model\n",
        "    print(\"\\n=== Training Baseline Model ===\")\n",
        "    baseline_model = create_baseline_model()\n",
        "    baseline_model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    baseline_start_time = time.time()\n",
        "    baseline_history = baseline_model.fit(\n",
        "        x_train, y_train,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        epochs=config.EPOCHS,\n",
        "        validation_split=config.VALIDATION_SPLIT,\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=5,\n",
        "                restore_best_weights=True\n",
        "            )\n",
        "        ],\n",
        "        verbose=1\n",
        "    )\n",
        "    baseline_training_time = time.time() - baseline_start_time\n",
        "\n",
        "    baseline_test_start_time = time.time()\n",
        "    baseline_loss, baseline_accuracy = baseline_model.evaluate(x_test, y_test, verbose=0)\n",
        "    baseline_test_time = time.time() - baseline_test_start_time\n",
        "\n",
        "    print(f\"Baseline Test Accuracy: {baseline_accuracy:.4f}\")\n",
        "    print(f\"Baseline Training Time: {baseline_training_time:.2f} seconds\")\n",
        "    print(f\"Baseline Inference Time: {baseline_test_time:.2f} seconds\")\n",
        "\n",
        "    # Analyze baseline results\n",
        "    print(\"\\n=== Analyzing Baseline Results ===\")\n",
        "    baseline_results = analyze_results(baseline_model, x_test, y_test)\n",
        "    baseline_results['training_time'] = baseline_training_time\n",
        "    baseline_results['test_time'] = baseline_test_time\n",
        "\n",
        "    # Plot baseline training history\n",
        "    plot_training_history(baseline_history)\n",
        "\n",
        "    # Create and train the custom model\n",
        "    print(\"\\n=== Training Local-Global Feature Model ===\")\n",
        "    custom_model = LocalGlobalDigitClassifier(config)\n",
        "\n",
        "    # Visualize some patches before training\n",
        "    visualize_patches(custom_model, x_train[:10])\n",
        "\n",
        "    # Train the custom model\n",
        "    custom_history, custom_accuracy, custom_training_time, custom_test_time = train_model(\n",
        "        custom_model, x_train, y_train, x_test, y_test, config\n",
        "    )\n",
        "\n",
        "    # Analyze custom model results\n",
        "    print(\"\\n=== Analyzing Local-Global Model Results ===\")\n",
        "    custom_results = analyze_results(custom_model, x_test, y_test)\n",
        "    custom_results['training_time'] = custom_training_time\n",
        "    custom_results['test_time'] = custom_test_time\n",
        "\n",
        "    # Plot custom model training history\n",
        "    plot_training_history(custom_history)\n",
        "\n",
        "    # Compare models\n",
        "    print(\"\\n=== Comparing Models ===\")\n",
        "    compare_models(custom_results, baseline_results)\n",
        "\n",
        "    # Generate mathematical model report\n",
        "    print(\"\\n=== Generating Mathematical Model Report ===\")\n",
        "    generate_model_report()\n",
        "\n",
        "    print(\"\\nExperiment complete. Results and visualizations saved to disk.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZZhmkKfg97G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}